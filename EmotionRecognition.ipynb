{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLFbdIf8j6pm",
        "outputId": "82cdd043-da21-4a19-e157-8a35563f56e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfuCJvORrxve"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bo4kn4pxYVm",
        "outputId": "7cbd8097-da2e-4197-f5f8-de330fb82f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "from os import path\n",
        "#import the necessary packages\n",
        "# define the base path to the emotion dataset\n",
        "BASE_PATH = '/content/gdrive/My Drive/EmotionRecognition'\n",
        "\n",
        "# use the base path to define the path to the input emotions file\n",
        "INPUT_PATH = path.sep.join([BASE_PATH, \"fer2013/fer2013.csv\"])\n",
        "NUM_CLASSES = 6\n",
        "\n",
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = path.sep.join([BASE_PATH, \"hdf5/train.hdf5\"])\n",
        "VAL_HDF5 = path.sep.join([BASE_PATH, \"hdf5/val.hdf5\"])\n",
        "TEST_HDF5 = path.sep.join([BASE_PATH, \"hdf5/test.hdf5\"])\n",
        "if os.path.exists(TRAIN_HDF5):\n",
        "  os.remove(TRAIN_HDF5)\n",
        "if os.path.exists(VAL_HDF5):\n",
        "  os.remove(VAL_HDF5)\n",
        "if os.path.exists(TEST_HDF5):\n",
        "  os.remove(TEST_HDF5)\n",
        "\n",
        "# define the batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# define the path to where output logs will be stored\n",
        "OUTPUT_PATH = path.sep.join([BASE_PATH, \"output\"])\n",
        "print(BASE_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/EmotionRecognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teAgFZr00uEN"
      },
      "source": [
        "## HDF5 Dataset Writer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BjudEu80zy5"
      },
      "source": [
        "# import the necessary packages\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "class HDF5DatasetWriter:\n",
        "\tdef __init__(self, dims, outputPath, dataKey=\"images\",\n",
        "\t\tbufSize=1000):\n",
        "\t\t# check to see if the output path exists, and if so, raise\n",
        "\t\t# an exception\n",
        "\t\tif os.path.exists(outputPath):\n",
        "\t\t\traise ValueError(\"The supplied `outputPath` already \"\n",
        "\t\t\t\t\"exists and cannot be overwritten. Manually delete \"\n",
        "\t\t\t\t\"the file before continuing.\", outputPath)\n",
        "\n",
        "\t\t# open the HDF5 database for writing and create two datasets:\n",
        "\t\t# one to store the images/features and another to store the\n",
        "\t\t# class labels\n",
        "\t\tself.db = h5py.File(outputPath, \"w\")\n",
        "\t\tself.data = self.db.create_dataset(dataKey, dims,\n",
        "\t\t\tdtype=\"float\")\n",
        "\t\tself.labels = self.db.create_dataset(\"labels\", (dims[0],),\n",
        "\t\t\tdtype=\"int\")\n",
        "\n",
        "\t\t# store the buffer size, then initialize the buffer itself\n",
        "\t\t# along with the index into the datasets\n",
        "\t\tself.bufSize = bufSize\n",
        "\t\tself.buffer = {\"data\": [], \"labels\": []}\n",
        "\t\tself.idx = 0\n",
        "\n",
        "\tdef add(self, rows, labels):\n",
        "\t\t# add the rows and labels to the buffer\n",
        "\t\tself.buffer[\"data\"].extend(rows)\n",
        "\t\tself.buffer[\"labels\"].extend(labels)\n",
        "\n",
        "\t\t# check to see if the buffer needs to be flushed to disk\n",
        "\t\tif len(self.buffer[\"data\"]) >= self.bufSize:\n",
        "\t\t\tself.flush()\n",
        "\n",
        "\tdef flush(self):\n",
        "\t\t# write the buffers to disk then reset the buffer\n",
        "\t\ti = self.idx + len(self.buffer[\"data\"])\n",
        "\t\tself.data[self.idx:i] = self.buffer[\"data\"]\n",
        "\t\tself.labels[self.idx:i] = self.buffer[\"labels\"]\n",
        "\t\tself.idx = i\n",
        "\t\tself.buffer = {\"data\": [], \"labels\": []}\n",
        "\n",
        "\tdef storeClassLabels(self, classLabels):\n",
        "\t\t# create a dataset to store the actual class label names,\n",
        "\t\t# then store the class labels\n",
        "\t\tdt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n",
        "\t\tlabelSet = self.db.create_dataset(\"label_names\",\n",
        "\t\t\t(len(classLabels),), dtype=dt)\n",
        "\t\tlabelSet[:] = classLabels\n",
        "\n",
        "\tdef close(self):\n",
        "\t\t# check to see if there are any other entries in the buffer\n",
        "\t\t# that need to be flushed to disk\n",
        "\t\tif len(self.buffer[\"data\"]) > 0:\n",
        "\t\t\tself.flush()\n",
        "\n",
        "\t\t# close the dataset\n",
        "\t\tself.db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne35Ev6mzwoR"
      },
      "source": [
        "## Build dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byz75Hzzzvho",
        "outputId": "1015d400-ea74-4685-9df8-7e2a1c3908e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# import the necessary packages\n",
        "# from config import emotion_config as config\n",
        "# from ImageUtilities.io import HDF5DatasetWriter\n",
        "import numpy as np\n",
        "\n",
        "# open the input file for reading (skipping the header), then\n",
        "# initialize the list of data and labels for the training,\n",
        "# validation, and testing sets\n",
        "print(\"[INFO] loading input data...\")\n",
        "f = open(INPUT_PATH)\n",
        "f.__next__() # f.next() for Python 2.7\n",
        "(trainImages, trainLabels) = ([], [])\n",
        "(valImages, valLabels) = ([], [])\n",
        "(testImages, testLabels) = ([], [])\n",
        "# loop over the rows in the input file\n",
        "for row in f:\n",
        "    # extract the label, image, and usage from the row\n",
        "    (label, image, usage) = row.strip().split(\",\")\n",
        "    label = int(label)\n",
        "    # if we are ignoring the \"disgust\" class there will be 6 total\n",
        "    # # class labels instead of 7\n",
        "    if NUM_CLASSES == 6:\n",
        "        # merge together the \"anger\" and \"disgust classes\n",
        "        if label == 1:\n",
        "            label = 0\n",
        "        # if label has a value greater than zero, subtract one from\n",
        "        # it to make all labels sequential (not required, but helps\n",
        "        # when interpreting results)\n",
        "        if label > 0:\n",
        "            label -= 1\n",
        "    \n",
        "    # reshape the flattened pixel list into a 48x48 (grayscale)\n",
        "    # image\n",
        "    image = np.array(image.split(\" \"), dtype=\"uint8\")\n",
        "    image = image.reshape((48, 48))\n",
        "\n",
        "    # check if we are examining a training image\n",
        "    if usage == \"Training\":\n",
        "        trainImages.append(image)\n",
        "        trainLabels.append(label)\n",
        "    # check if this is a validation image\n",
        "    elif usage == \"PrivateTest\":\n",
        "        valImages.append(image)\n",
        "        valLabels.append(label)\n",
        "    # otherwise, this must be a testing image\n",
        "    else:\n",
        "        testImages.append(image)\n",
        "        testLabels.append(label)\n",
        "\n",
        "# construct a list pairing the training, validation, and testing\n",
        "# images along with their corresponding labels and output HDF5\n",
        "# files\n",
        "datasets = [\n",
        "    (trainImages, trainLabels, TRAIN_HDF5),\n",
        "    (valImages, valLabels, VAL_HDF5),\n",
        "    (testImages, testLabels, TEST_HDF5)]\n",
        "\n",
        "# Loop over the dataset tuples\n",
        "\n",
        "for (images, labels, outputPath) in datasets:\n",
        "    # create HDF5 writer\n",
        "    print(\"[INFO] building {}...\".format(outputPath))\n",
        "    writer = HDF5DatasetWriter((len(images), 48, 48), outputPath)\n",
        "    \n",
        "    # loop over the image and add them to the dataset\n",
        "    # \n",
        "    for (image, label) in zip(images, labels):\n",
        "        writer.add([image], [label])\n",
        "        \n",
        "    # close the HDF5 writer\n",
        "    writer.close()\n",
        "    # close the input file\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading input data...\n",
            "[INFO] building /content/gdrive/My Drive/EmotionRecognition/hdf5/train.hdf5...\n",
            "[INFO] building /content/gdrive/My Drive/EmotionRecognition/hdf5/val.hdf5...\n",
            "[INFO] building /content/gdrive/My Drive/EmotionRecognition/hdf5/test.hdf5...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAcQb4CyzL3G"
      },
      "source": [
        "## VGGNet for Emotion Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrlo-Ia4zTYo"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import ELU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "num_features = 64\n",
        "class EmotionVGGNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\t\t########################## Adrian's Model ######################\n",
        "\t\t# Block #1: first CONV => RELU => CONV => RELU => POOL\n",
        "\t\t# layer set\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n",
        "\t\t\tkernel_initializer=\"he_normal\", input_shape=inputShape))\n",
        "\t\tmodel.add(ELU())\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), kernel_initializer=\"he_normal\",\n",
        "\t\t\tpadding=\"same\"))\n",
        "\t\tmodel.add(ELU())\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.55))\n",
        "\n",
        "\t\t# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
        "\t\t# layer set\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), kernel_initializer=\"he_normal\",\n",
        "\t\t\tpadding=\"same\"))\n",
        "\t\tmodel.add(ELU())\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(64, (3, 3), kernel_initializer=\"he_normal\",\n",
        "\t\t\tpadding=\"same\"))\n",
        "\t\tmodel.add(ELU())\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
        "\t\t# layer set\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), kernel_initializer=\"he_normal\",\n",
        "\t\t\tpadding=\"same\"))\n",
        "\t\tmodel.add(ELU())\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(128, (3, 3), kernel_initializer=\"he_normal\",\n",
        "\t\t\tpadding=\"same\"))\n",
        "\t\tmodel.add(ELU())\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.45))\n",
        "\n",
        "\t\t# Block #4: first set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
        "\t\tmodel.add(ELU())\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.65))\n",
        "\n",
        "\t\t# Block #6: second set of FC => RELU layers\n",
        "\t\tmodel.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
        "\t\tmodel.add(ELU())\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.7))\n",
        "\n",
        "\t\t# Block #7: softmax classifier\n",
        "\t\tmodel.add(Dense(classes, kernel_initializer=\"he_normal\"))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\t\t\n",
        "\t\t########################## Kaggle Model ######################\n",
        "\t\t# #module 1\n",
        "\t\t# model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), kernel_initializer=\"he_normal\", input_shape=inputShape))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t# model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), kernel_initializer=\"he_normal\", padding='same'))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\t\t# model.add(Dropout(0.15))\n",
        "\n",
        "\t\t# #module 2\n",
        "\t\t# model.add(Conv2D(2*num_features, kernel_size=(3, 3), kernel_initializer=\"he_normal\", padding='same'))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t# model.add(Conv2D(2*num_features, kernel_size=(3, 3), kernel_initializer=\"he_normal\", padding='same'))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\t\t# model.add(Dropout(0.15))\n",
        "\n",
        "\t\t# #module 3\n",
        "\t\t# model.add(Conv2D(num_features, kernel_size=(3, 3), kernel_initializer=\"he_normal\", padding='same'))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t# model.add(Conv2D(num_features, kernel_size=(3, 3), kernel_initializer=\"he_normal\", padding='same'))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization(axis=chanDim))\n",
        "\t\t# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\t\t# model.add(Dropout(0.15))\n",
        "\n",
        "\t\t# #flatten\n",
        "\t\t# model.add(Flatten())\n",
        "\n",
        "\t\t# #dense 1\n",
        "\t\t# model.add(Dense(2*2*num_features, kernel_initializer=\"he_normal\"))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization())\n",
        "\t\t# model.add(Dropout(0.2))\n",
        "\t\n",
        "\t\t# #dense 2\n",
        "\t\t# model.add(Dense(2*2*num_features, kernel_initializer=\"he_normal\"))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization())\n",
        "\t\t# model.add(Dropout(0.4))\n",
        "\t\n",
        "\t\t# #dense 3\n",
        "\t\t# model.add(Dense(2*num_features, kernel_initializer=\"he_normal\"))\n",
        "\t\t# # model.add(Activation('relu'))\n",
        "\t\t# model.add(ELU())\n",
        "\t\t# model.add(BatchNormalization())\n",
        "\t\t# model.add(Dropout(0.4))\n",
        "\t\n",
        "\t\t# #output layer\n",
        "\t\t# model.add(Dense(NUM_CLASSES, kernel_initializer=\"he_normal\", activation='softmax'))\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\t# visualize the network architecture\n",
        "\tfrom tensorflow.keras.utils import plot_model\n",
        "\tfrom tensorflow.keras.regularizers import l2\n",
        "\tmodel = EmotionVGGNet.build(48, 48, 1, 6)\n",
        "\tplot_model(model, to_file=\"model.png\", show_shapes=True,\n",
        "\t\tshow_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ3HtiIa2H-X"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFxAWYiI92YR"
      },
      "source": [
        "### dataset generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzqVDLen96kH"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "class HDF5DatasetGenerator:\n",
        "\tdef __init__(self, dbPath, batchSize, preprocessors=None,\n",
        "\t\taug=None, binarize=True, classes=2):\n",
        "\t\t# store the batch size, preprocessors, and data augmentor,\n",
        "\t\t# whether or not the labels should be binarized, along with\n",
        "\t\t# the total number of classes\n",
        "\t\tself.batchSize = batchSize\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\t\tself.aug = aug\n",
        "\t\tself.binarize = binarize\n",
        "\t\tself.classes = classes\n",
        "\n",
        "\t\t# open the HDF5 database for reading and determine the total\n",
        "\t\t# number of entries in the database\n",
        "\t\tself.db = h5py.File(dbPath)\n",
        "\t\tself.numImages = self.db[\"labels\"].shape[0]\n",
        "\n",
        "\tdef generator(self, passes=np.inf):\n",
        "\t\t# initialize the epoch count\n",
        "\t\tepochs = 0\n",
        "\n",
        "\t\t# keep looping infinitely -- the model will stop once we have\n",
        "\t\t# reach the desired number of epochs\n",
        "\t\twhile epochs < passes:\n",
        "\t\t\t# loop over the HDF5 dataset\n",
        "\t\t\tfor i in np.arange(0, self.numImages, self.batchSize):\n",
        "\t\t\t\t# extract the images and labels from the HDF dataset\n",
        "\t\t\t\timages = self.db[\"images\"][i: i + self.batchSize]\n",
        "\t\t\t\tlabels = self.db[\"labels\"][i: i + self.batchSize]\n",
        "\n",
        "\t\t\t\t# check to see if the labels should be binarized\n",
        "\t\t\t\tif self.binarize:\n",
        "\t\t\t\t\tlabels = to_categorical(labels,\n",
        "\t\t\t\t\t\tself.classes)\n",
        "\n",
        "\t\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t\t# initialize the list of processed images\n",
        "\t\t\t\t\tprocImages = []\n",
        "\n",
        "\t\t\t\t\t# loop over the images\n",
        "\t\t\t\t\tfor image in images:\n",
        "\t\t\t\t\t\t# loop over the preprocessors and apply each\n",
        "\t\t\t\t\t\t# to the image\n",
        "\t\t\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t\t\t\t# update the list of processed images\n",
        "\t\t\t\t\t\tprocImages.append(image)\n",
        "\n",
        "\t\t\t\t\t# update the images array to be the processed\n",
        "\t\t\t\t\t# images\n",
        "\t\t\t\t\timages = np.array(procImages)\n",
        "\n",
        "\t\t\t\t# if the data augmenator exists, apply it\n",
        "\t\t\t\tif self.aug is not None:\n",
        "\t\t\t\t\t(images, labels) = next(self.aug.flow(images,\n",
        "\t\t\t\t\t\tlabels, batch_size=self.batchSize))\n",
        "\n",
        "\t\t\t\t# yield a tuple of images and labels\n",
        "\t\t\t\tyield (images, labels)\n",
        "\n",
        "\t\t\t# increment the total number of epochs\n",
        "\t\t\tepochs += 1\n",
        "\n",
        "\tdef close(self):\n",
        "\t\t# close the database\n",
        "\t\tself.db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGtvcU4q2rGv"
      },
      "source": [
        "### Image to Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzJm_z6e2dl3"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZLrU29U2wM5"
      },
      "source": [
        "### Epoch Checkpointer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9E5feMg3BVP"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "import os\n",
        "\n",
        "class EpochCheckpoint(Callback):\n",
        "\tdef __init__(self, outputPath, every=5, startAt=0):\n",
        "\t\t# call the parent constructor\n",
        "\t\tsuper(Callback, self).__init__()\n",
        "\n",
        "\t\t# store the base output path for the model, the number of\n",
        "\t\t# epochs that must pass before the model is serialized to\n",
        "\t\t# disk and the current epoch value\n",
        "\t\tself.outputPath = outputPath\n",
        "\t\tself.every = every\n",
        "\t\tself.intEpoch = startAt\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# check to see if the model should be serialized to disk\n",
        "\t\tif (self.intEpoch + 1) % self.every == 0:\n",
        "\t\t\tp = os.path.sep.join([self.outputPath,\n",
        "\t\t\t\t\"epoch_{}.hdf5\".format(self.intEpoch + 1)])\n",
        "\t\t\tself.model.save(p, overwrite=True)\n",
        "\n",
        "\t\t# increment the internal epoch counter\n",
        "\t\tself.intEpoch += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "943Spgoy3IXF"
      },
      "source": [
        "### Training Monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMXmB_vV3Uw-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8Pb5Kh2Qkx",
        "outputId": "157c4aa5-6e76-4130-b5f9-fda38988f7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# USAGE\n",
        "# python train_recognizer.py --checkpoints fer2013/checkpoints\n",
        "# python train_recognizer.py --checkpoints fer2013/checkpoints --model fer2013/checkpoints/epoch_20.hdf5 \\\n",
        "#   --start-epoch 20\n",
        " \n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "from tensorflow.keras.callbacks import BaseLogger\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "# import the necessary packages\n",
        "# from config import emotion_config as config\n",
        "# from ImageUtilities.preprocessing import ImageToArrayPreprocessor\n",
        "# from ImageUtilities.callbacks import TrainingMonitor\n",
        "# from ImageUtilities.callbacks import EpochCheckpoint\n",
        "# from ImageUtilities.io import HDF5DatasetGenerator\n",
        "# from ImageUtilities.nn.conv import EmotionVGGNet\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow.keras.backend as K\n",
        "# import argparse\n",
        "import os\n",
        " \n",
        "# construct the argument parse and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-c\", \"--checkpoints\", required=True,\n",
        "#   help=\"path to output checkpoint directory\")\n",
        "# ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "#   help=\"path to *specific* model checkpoint to load\")\n",
        "# ap.add_argument(\"-s\", \"--start-epoch\", type=int, default=0,\n",
        "#   help=\"epoch to restart training at\")\n",
        "# args = vars(ap.parse_args())\n",
        " \n",
        "##################### Training Monitor #########################\n",
        "class TrainingMonitor(BaseLogger):\n",
        "    def __init__(self, figPath, jsonPath=None, startAt=0):\n",
        "        # store the output path for the figure, the path to the JSON\n",
        "        # serialized file, and the starting epoch\n",
        "        super(TrainingMonitor, self).__init__()\n",
        "        self.figPath = figPath\n",
        "        self.jsonPath = jsonPath\n",
        "        self.startAt = startAt\n",
        " \n",
        "    def on_train_begin(self, logs={}):\n",
        "        # initialize the history dictionary\n",
        "        self.H = {}\n",
        " \n",
        "        # if the JSON history path exists, load the training history\n",
        "        if self.jsonPath is not None:\n",
        "            if os.path.exists(self.jsonPath):\n",
        "                self.H = json.loads(open(self.jsonPath).read())\n",
        " \n",
        "                # check to see if a starting epoch was supplied\n",
        "                if self.startAt > 0:\n",
        "                    # loop over the entries in the history log and\n",
        "                    # trim any entries that are past the starting\n",
        "                    # epoch\n",
        "                    for k in self.H.keys():\n",
        "                        self.H[k] = self.H[k][:self.startAt]\n",
        " \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # loop over the logs and update the loss, accuracy, etc.\n",
        "        # for the entire training process\n",
        "        for (k, v) in logs.items():\n",
        "            l = self.H.get(k, [])\n",
        "            # print(\"l : \", l)\n",
        "            # print(\"k : \", k)\n",
        "            # print(\"v: \", v)\n",
        "            l.append(str(v))\n",
        "            self.H[k] = l\n",
        " \n",
        "        # check to see if the training history should be serialized\n",
        "        # to file\n",
        "        if self.jsonPath is not None:\n",
        "            f = open(self.jsonPath, \"w\")\n",
        "            f.write(json.dumps(self.H))\n",
        "            f.close()\n",
        " \n",
        "        # ensure at least two epochs have passed before plotting\n",
        "        # (epoch starts at zero)\n",
        "        if len(self.H[\"loss\"]) > 1:\n",
        "            # plot the training loss and accuracy\n",
        "            N = np.arange(0, len(self.H[\"loss\"]))\n",
        "            plt.style.use(\"ggplot\")\n",
        "            plt.figure()\n",
        "            plt.plot(N, np.array(self.H[\"loss\"]).astype(np.float), label=\"train_loss\")\n",
        "            plt.plot(N, np.array(self.H[\"val_loss\"]).astype(np.float), label=\"val_loss\")\n",
        "            plt.plot(N, np.array(self.H[\"accuracy\"]).astype(np.float), label=\"train_acc\")\n",
        "            plt.plot(N, np.array(self.H[\"val_accuracy\"]).astype(float), label=\"val_acc\")\n",
        "            plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n",
        "                len(self.H[\"loss\"])))\n",
        "            plt.xlabel(\"Epoch #\")\n",
        "            plt.ylabel(\"Loss/Accuracy\")\n",
        "            plt.legend()\n",
        " \n",
        "            # save the figure\n",
        "            plt.savefig(self.figPath)\n",
        "            plt.close()\n",
        "#################### Training Monitor Ends here ###################\n",
        "checkpoints = '/content/gdrive/My Drive/EmotionRecognition/checkpoints/'\n",
        "# checkpoint_model = '/content/gdrive/My Drive/EmotionRecognition/checkpoints/epoch_40.hdf5'\n",
        "checkpoint_model = None\n",
        " \n",
        "# learning rate schedule\n",
        "def my_schedule(epoch):\n",
        "    if epoch >= 0 and epoch < 40:\n",
        "        lrate = 1e-3\n",
        "        print(\"[Info] Learning rate: \", K.get_value(model.optimizer.lr))\n",
        "    if epoch >= 41 and epoch < 60:\n",
        "        lrate = 1e-4\n",
        "        print(\"[Info] Learning rate: \", K.get_value(model.optimizer.lr))\n",
        "    if epoch >= 61 and epoch < 75:\n",
        "        lrate = 1e-5\n",
        "        print(\"[Info] Learning rate: \", K.get_value(model.optimizer.lr))\n",
        "    return lrate\n",
        " \n",
        " \n",
        "# construct the training and testing image generators for data\n",
        "# augmentation, then initialize the image preprocessor\n",
        "trainAug = ImageDataGenerator(rotation_range=10, zoom_range=0.1,\n",
        "    horizontal_flip=True, height_shift_range=0.1, rescale=1 / 255.0, fill_mode=\"nearest\")\n",
        "valAug = ImageDataGenerator(rotation_range=10, zoom_range=0.1,\n",
        "    horizontal_flip=True, height_shift_range=0.1, rescale=1 / 255.0, fill_mode=\"nearest\")\n",
        "iap = ImageToArrayPreprocessor()\n",
        " \n",
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, BATCH_SIZE,\n",
        "    aug=trainAug, preprocessors=[iap], classes=NUM_CLASSES)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, BATCH_SIZE,\n",
        "    aug=valAug, preprocessors=[iap], classes=NUM_CLASSES)\n",
        " \n",
        "start_epoch = 0\n",
        "INIT_LR = 1e-3\n",
        "EPOCHS = 75\n",
        "BATCHES_PER_EPOCH = trainGen.numImages // BATCH_SIZE\n",
        "LR_DECAY = (1./0.01-1)/ BATCHES_PER_EPOCH\n",
        "# if there is no specific model checkpoint supplied, then initialize\n",
        "# the network and compile the model\n",
        "if checkpoint_model is None:\n",
        "    print(\"[INFO] compiling model...\")\n",
        "    model = EmotionVGGNet.build(width=48, height=48, depth=1,\n",
        "        classes=NUM_CLASSES)\n",
        "    # opt = Adam(lr=INIT_LR, decay=LR_DECAY)\n",
        "    opt = Adam(lr=INIT_LR, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "        metrics=[\"accuracy\"])\n",
        " \n",
        "# otherwise, load the checkpoint from disk\n",
        "else:\n",
        "    print(\"[INFO] loading {}...\".format(checkpoint_model))\n",
        "    model = load_model(checkpoint_model)\n",
        " \n",
        "    # update the learning rate\n",
        "    print(\"[INFO] old learning rate: {}\".format(\n",
        "        K.get_value(model.optimizer.lr)))\n",
        "    K.set_value(model.optimizer.lr, INIT_LR)\n",
        "    print(\"[INFO] new learning rate: {}\".format(\n",
        "        K.get_value(model.optimizer.lr)))\n",
        " \n",
        "# construct the set of callbacks\n",
        "figPath = os.path.sep.join([OUTPUT_PATH,\n",
        "    \"vggnet_emotion.png\"])\n",
        "jsonPath = os.path.sep.join([OUTPUT_PATH,\n",
        "    \"vggnet_emotion.json\"])\n",
        "lrate = LearningRateScheduler(my_schedule)\n",
        "callbacks = [\n",
        "    EpochCheckpoint(checkpoints, every=5,\n",
        "        startAt=start_epoch),\n",
        "    TrainingMonitor(figPath, jsonPath=jsonPath,\n",
        "        startAt=start_epoch),\n",
        "        lrate]\n",
        " \n",
        "# train the network\n",
        "model.fit_generator(\n",
        "    trainGen.generator(),\n",
        "    steps_per_epoch=BATCHES_PER_EPOCH,\n",
        "    validation_data=valGen.generator(),\n",
        "    validation_steps=valGen.numImages // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    max_queue_size=BATCH_SIZE * 2,\n",
        "    callbacks=callbacks, verbose=1)\n",
        " \n",
        "# close the databases\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n",
            "WARNING:tensorflow:From <ipython-input-16-9e76bea4afbd>:186: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 1/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 2.5360 - accuracy: 0.2050 - val_loss: 2.0082 - val_accuracy: 0.2584 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 2/75\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.8287 - accuracy: 0.2571 - val_loss: 1.7005 - val_accuracy: 0.2882 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 3/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.6642 - accuracy: 0.3188 - val_loss: 1.6005 - val_accuracy: 0.3527 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 4/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.6020 - accuracy: 0.3477 - val_loss: 1.5307 - val_accuracy: 0.3820 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 5/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.5533 - accuracy: 0.3748 - val_loss: 1.4486 - val_accuracy: 0.4155 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 6/75\n",
            "224/224 [==============================] - 15s 67ms/step - loss: 1.4956 - accuracy: 0.4017 - val_loss: 1.4140 - val_accuracy: 0.4425 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 7/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.4415 - accuracy: 0.4263 - val_loss: 1.3436 - val_accuracy: 0.4707 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 8/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.3969 - accuracy: 0.4493 - val_loss: 1.3186 - val_accuracy: 0.4835 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 9/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.3589 - accuracy: 0.4649 - val_loss: 1.2681 - val_accuracy: 0.4997 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 10/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.3356 - accuracy: 0.4759 - val_loss: 1.2283 - val_accuracy: 0.5226 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 11/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.3059 - accuracy: 0.4919 - val_loss: 1.2250 - val_accuracy: 0.5285 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 12/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.2754 - accuracy: 0.5045 - val_loss: 1.1807 - val_accuracy: 0.5430 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 13/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.2634 - accuracy: 0.5095 - val_loss: 1.1723 - val_accuracy: 0.5438 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 14/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.2388 - accuracy: 0.5200 - val_loss: 1.1409 - val_accuracy: 0.5628 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 15/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.2201 - accuracy: 0.5298 - val_loss: 1.1222 - val_accuracy: 0.5650 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 16/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.2058 - accuracy: 0.5382 - val_loss: 1.1239 - val_accuracy: 0.5580 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 17/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.1929 - accuracy: 0.5417 - val_loss: 1.0952 - val_accuracy: 0.5781 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 18/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.1767 - accuracy: 0.5519 - val_loss: 1.1349 - val_accuracy: 0.5578 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 19/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 1.1582 - accuracy: 0.5594 - val_loss: 1.0595 - val_accuracy: 0.5882 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 20/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.1509 - accuracy: 0.5645 - val_loss: 1.0669 - val_accuracy: 0.5806 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 21/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.1374 - accuracy: 0.5697 - val_loss: 1.0562 - val_accuracy: 0.5910 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 22/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.1267 - accuracy: 0.5730 - val_loss: 1.0409 - val_accuracy: 0.6021 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 23/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.1254 - accuracy: 0.5736 - val_loss: 1.0396 - val_accuracy: 0.5971 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 24/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.1088 - accuracy: 0.5828 - val_loss: 1.0446 - val_accuracy: 0.5957 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 25/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.1050 - accuracy: 0.5801 - val_loss: 1.0219 - val_accuracy: 0.6077 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 26/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.0962 - accuracy: 0.5871 - val_loss: 1.0023 - val_accuracy: 0.6085 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 27/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.0931 - accuracy: 0.5859 - val_loss: 1.0105 - val_accuracy: 0.6113 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 28/75\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 1.0856 - accuracy: 0.5919 - val_loss: 1.0271 - val_accuracy: 0.5999 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 29/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.0801 - accuracy: 0.5971 - val_loss: 0.9932 - val_accuracy: 0.6138 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 30/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.0704 - accuracy: 0.5966 - val_loss: 0.9878 - val_accuracy: 0.6225 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 31/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.0660 - accuracy: 0.6006 - val_loss: 1.0006 - val_accuracy: 0.6203 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 32/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.0548 - accuracy: 0.6030 - val_loss: 0.9802 - val_accuracy: 0.6295 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 33/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.0532 - accuracy: 0.6068 - val_loss: 0.9687 - val_accuracy: 0.6283 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 34/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.0529 - accuracy: 0.6096 - val_loss: 0.9947 - val_accuracy: 0.6236 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 35/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.0389 - accuracy: 0.6133 - val_loss: 0.9623 - val_accuracy: 0.6303 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 36/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.0404 - accuracy: 0.6105 - val_loss: 0.9590 - val_accuracy: 0.6353 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 37/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 1.0320 - accuracy: 0.6136 - val_loss: 0.9593 - val_accuracy: 0.6325 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 38/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.0296 - accuracy: 0.6155 - val_loss: 0.9819 - val_accuracy: 0.6278 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 39/75\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 1.0294 - accuracy: 0.6147 - val_loss: 0.9643 - val_accuracy: 0.6362 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 40/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 1.0187 - accuracy: 0.6207 - val_loss: 1.0021 - val_accuracy: 0.6147 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 41/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 1.0157 - accuracy: 0.6209 - val_loss: 1.0086 - val_accuracy: 0.6141 - lr: 0.0010\n",
            "[Info] Learning rate:  0.001\n",
            "Epoch 42/75\n",
            "224/224 [==============================] - 15s 68ms/step - loss: 0.9998 - accuracy: 0.6260 - val_loss: 0.9368 - val_accuracy: 0.6454 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 43/75\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 0.9978 - accuracy: 0.6288 - val_loss: 0.9283 - val_accuracy: 0.6468 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 44/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9890 - accuracy: 0.6349 - val_loss: 0.9302 - val_accuracy: 0.6367 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 45/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9778 - accuracy: 0.6379 - val_loss: 0.9336 - val_accuracy: 0.6409 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 46/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9762 - accuracy: 0.6384 - val_loss: 0.9259 - val_accuracy: 0.6526 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 47/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9762 - accuracy: 0.6391 - val_loss: 0.9302 - val_accuracy: 0.6465 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 48/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9734 - accuracy: 0.6385 - val_loss: 0.9296 - val_accuracy: 0.6456 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 49/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9684 - accuracy: 0.6417 - val_loss: 0.9245 - val_accuracy: 0.6512 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 50/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9678 - accuracy: 0.6418 - val_loss: 0.9268 - val_accuracy: 0.6484 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 51/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9692 - accuracy: 0.6416 - val_loss: 0.9275 - val_accuracy: 0.6470 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 52/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9694 - accuracy: 0.6398 - val_loss: 0.9211 - val_accuracy: 0.6518 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 53/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9714 - accuracy: 0.6391 - val_loss: 0.9214 - val_accuracy: 0.6498 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 54/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9705 - accuracy: 0.6425 - val_loss: 0.9191 - val_accuracy: 0.6507 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 55/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9644 - accuracy: 0.6423 - val_loss: 0.9162 - val_accuracy: 0.6593 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 56/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9666 - accuracy: 0.6442 - val_loss: 0.9228 - val_accuracy: 0.6493 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 57/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9633 - accuracy: 0.6450 - val_loss: 0.9142 - val_accuracy: 0.6523 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 58/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9617 - accuracy: 0.6435 - val_loss: 0.9117 - val_accuracy: 0.6529 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 59/75\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 0.9571 - accuracy: 0.6479 - val_loss: 0.9101 - val_accuracy: 0.6588 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 60/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9572 - accuracy: 0.6458 - val_loss: 0.9040 - val_accuracy: 0.6560 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 61/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9572 - accuracy: 0.6474 - val_loss: 0.9101 - val_accuracy: 0.6537 - lr: 1.0000e-04\n",
            "[Info] Learning rate:  1e-04\n",
            "Epoch 62/75\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 0.9546 - accuracy: 0.6444 - val_loss: 0.9163 - val_accuracy: 0.6518 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 63/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9478 - accuracy: 0.6484 - val_loss: 0.9069 - val_accuracy: 0.6590 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 64/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9516 - accuracy: 0.6475 - val_loss: 0.9195 - val_accuracy: 0.6501 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 65/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9512 - accuracy: 0.6480 - val_loss: 0.9247 - val_accuracy: 0.6509 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 66/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9539 - accuracy: 0.6465 - val_loss: 0.9149 - val_accuracy: 0.6490 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 67/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9530 - accuracy: 0.6480 - val_loss: 0.9163 - val_accuracy: 0.6521 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 68/75\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 0.9516 - accuracy: 0.6468 - val_loss: 0.9100 - val_accuracy: 0.6554 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 69/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9524 - accuracy: 0.6489 - val_loss: 0.9088 - val_accuracy: 0.6468 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 70/75\n",
            "224/224 [==============================] - 16s 70ms/step - loss: 0.9479 - accuracy: 0.6505 - val_loss: 0.9210 - val_accuracy: 0.6512 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 71/75\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 0.9557 - accuracy: 0.6495 - val_loss: 0.9138 - val_accuracy: 0.6562 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 72/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9530 - accuracy: 0.6467 - val_loss: 0.9134 - val_accuracy: 0.6540 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 73/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9517 - accuracy: 0.6483 - val_loss: 0.9226 - val_accuracy: 0.6515 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 74/75\n",
            "224/224 [==============================] - 15s 69ms/step - loss: 0.9514 - accuracy: 0.6496 - val_loss: 0.9143 - val_accuracy: 0.6554 - lr: 1.0000e-05\n",
            "[Info] Learning rate:  1e-05\n",
            "Epoch 75/75\n",
            "224/224 [==============================] - 16s 69ms/step - loss: 0.9483 - accuracy: 0.6529 - val_loss: 0.9117 - val_accuracy: 0.6557 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}